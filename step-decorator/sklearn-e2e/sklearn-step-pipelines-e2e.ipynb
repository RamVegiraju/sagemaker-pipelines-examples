{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ce004b-4e1c-4c59-bb99-4657615b8cad",
   "metadata": {},
   "source": [
    "## SKLearn E2E Step Decorator Pipelines\n",
    "\n",
    "Example of lifting and shifting local Python code with the SageMaker Pipelines step decorator. Taking a local SKLearn example, where we create a dummy dataset, run model training, and perform sample inference/evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda5d94-e570-4e90-8dcc-2495e8a78a76",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3f486-2ce7-4b0b-bafc-4b3608b8bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ec74c-fa44-4dcb-9914-f91cda2bf3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set path to config file\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f016d52-c78f-4625-aed1-28eb2b67e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "instance_type = ParameterString(name=\"TrainInstanceType\", default_value=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c97743-b8cc-44ff-8ced-d757d3ca0015",
   "metadata": {},
   "source": [
    "### Step Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b8d56-5798-41a3-91b5-7fcd58b63f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one\n",
    "@step(\n",
    "    name = \"preprocess\",\n",
    "    instance_type = instance_type,\n",
    "    keep_alive_period_in_seconds=300\n",
    ")\n",
    "def create_data() -> tuple:\n",
    "    import numpy as np\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(100, 1)\n",
    "    y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)\n",
    "    data = (X,y)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b5031-2bbc-449d-8a22-1d9d72c6b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step two\n",
    "@step(\n",
    "    name = \"training\",\n",
    "    instance_type = instance_type,\n",
    "    keep_alive_period_in_seconds=300\n",
    ")\n",
    "def train_model(data: tuple) -> tuple:\n",
    "    import joblib\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    import boto3\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # unique bucket name\n",
    "    bucket_name = \"unique-bucket-step-pipelines-example-two\"\n",
    "    # create s3 bucket\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "    # unpack data\n",
    "    X = data[0]\n",
    "    y = data[1]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create a Linear Regression model\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Serialize trained model for inference\n",
    "    model_filename = \"model.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "\n",
    "    # Upload model artifact to s3\n",
    "    s3_file_name = \"model-artifacts/model.joblib\" #key to store model artifacts\n",
    "    s3.upload_file(model_filename, bucket_name, s3_file_name)\n",
    "    artifacts = (model_filename, bucket_name, s3_file_name, X_test, y_test)\n",
    "    return artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50104e5-f4ec-49c9-b750-c469769598f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step three\n",
    "@step(\n",
    "    name = \"inference_evaluation\",\n",
    "    instance_type = instance_type,\n",
    "    keep_alive_period_in_seconds=300\n",
    ")\n",
    "def model_inference(artifacts: tuple) -> float:\n",
    "    import joblib\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import numpy as np\n",
    "    import boto3\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    \n",
    "    # load up artifacts from previous step\n",
    "    model_filename = artifacts[0]\n",
    "    bucket_name = artifacts[1]\n",
    "    s3_file_name = artifacts[2]\n",
    "    X_test = artifacts[3]\n",
    "    y_test = artifacts[4]\n",
    "\n",
    "    # download model.joblib\n",
    "    s3.download_file(bucket_name, s3_file_name, model_filename)\n",
    "\n",
    "    # model loading + inference\n",
    "    serialized_model = joblib.load(model_filename)\n",
    "    preds = serialized_model.predict(X_test)\n",
    "\n",
    "    # evaluation\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c9a00-ca5b-4a6f-a703-2cfa315b6a6f",
   "metadata": {},
   "source": [
    "### Pipeline Orchestration and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d7607-c5f5-4515-87ff-8a4ca36f0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitch together pipeline\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "data = create_data()\n",
    "artifacts = train_model(data)\n",
    "rmse = model_inference(artifacts)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"sklearn-pipeline\",\n",
    "    parameters=[\n",
    "        instance_type\n",
    "    ],\n",
    "    steps=[\n",
    "        rmse,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f3e1f-1115-4dd0-8cfb-cf013123debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "execution.describe()\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a1056b-f6a9-400a-a65a-48264ffc0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
